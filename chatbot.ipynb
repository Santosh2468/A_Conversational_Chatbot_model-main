{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "chatboot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLUoC49zXoJN"
      },
      "source": [
        "# Building A Conversational Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upw5kp56XmX9"
      },
      "source": [
        "## Aim of the Project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZuG7e-xXkfC"
      },
      "source": [
        "Aim of the project is to build an intelligent\n",
        "conversational chatbot, Riki, that can understand\n",
        "complex queries from the user and intelligently respond."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfSUJoRiYFea"
      },
      "source": [
        "## Background\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC0iCBs7X9Rm"
      },
      "source": [
        "R-Intelligence Inc., an AI startup, has partnered with an online chat and discussion website\n",
        "bluedit.io. They have an average of over 5 million active customers across the globe and more\n",
        "than 100,000 active chat rooms. Due to the increased traffic, they are looking at improving\n",
        "their user experience with a chatbot moderator, which helps them engage in a meaningful\n",
        "conversation and keeps them updated on trending topics, while merely chatting with Riki, a\n",
        "chatbot. The Artificial Intelligence-powered chat experience provides easy access to\n",
        "information and a host of options to the customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obrXrAsoV7pJ"
      },
      "source": [
        "## Dataset description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUUdp0-SWBIE"
      },
      "source": [
        "Cornell Movie-Dialogs Corpus\n",
        "A large metadata-rich collection of fictional conversations extracted from raw movie scripts. (220,579 conversational exchanges between 10,292 pairs of movie characters in 617 movies).\n",
        "\n",
        "Distributed together with: Chameleons in Imagined Conversations: A new Approach to Understanding Coordination of Linguistic Style in Dialogs. Cristian Danescu-Niculescu-Mizil and Lillian Lee. Cognitive Modeling and Computational Linguistics Workshop at ACL 2011.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true,
        "id": "ai8i3NCcVnwI"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Activation, dot, concatenate\n",
        "\n",
        "INPUT_LENGTH = 20\n",
        "OUTPUT_LENGTH = 22\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgHcWS8uXPX7"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fQWJs-rYDxc"
      },
      "source": [
        "txt=open('/content/movie_lines_cleaned.txt','r').readlines()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qfq7LXERRkj5",
        "outputId": "983d66c9-ead8-4aa2-a36f-a62d6f7fc303"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9avRYUmXTm-"
      },
      "source": [
        "## Clean text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "85f3e1713f0620f066f5a5ee34b31c9a73d768fa",
        "id": "bc27Y2uZVnwN"
      },
      "source": [
        "def clean_text(text):\n",
        "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|]\", \"\", text)\n",
        "    text = \" \".join(text.split())\n",
        "    return text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L61pY6LTbG2z"
      },
      "source": [
        "#split the data to questions and answers\n",
        "questions=[]\n",
        "answers=[]\n",
        "for index,sent in enumerate(txt):\n",
        "  if index%2==0:\n",
        "    questions.append(sent)\n",
        "  else:\n",
        "    answers.append(sent)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "162b9f09f2e3e50ed92f157a5ce0faf3bb0d0019",
        "id": "AghALAw6VnwO"
      },
      "source": [
        "# Clean the data\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))\n",
        "clean_answers = []    \n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I182zdb9d-AD"
      },
      "source": [
        "last_que=clean_questions.pop() # to balance data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "b1df72b880bbf79a8043ae3b608d37e89b5807e9",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W2rA4bqVnwO",
        "outputId": "f31d07fb-3e04-4abe-ec18-6854eb0bbc78"
      },
      "source": [
        "# Find the length of sentences (not using nltk due to processing speed)\n",
        "lengths = []\n",
        "# lengths.append([len(nltk.word_tokenize(sent)) for sent in clean_questions]) #nltk approach\n",
        "for question in clean_questions:\n",
        "    lengths.append(len(question.split()))\n",
        "for answer in clean_answers:\n",
        "    lengths.append(len(answer.split()))\n",
        "# Create a dataframe so that the values can be inspected\n",
        "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
        "print(np.percentile(lengths, 80))\n",
        "print(np.percentile(lengths, 85))\n",
        "print(np.percentile(lengths, 90))\n",
        "print(np.percentile(lengths, 95))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16.0\n",
            "19.0\n",
            "24.0\n",
            "33.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0035c49ba3da36e903a600b22c737e6a5c34de6e",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0bXq3ObVnwP",
        "outputId": "03e5a3ce-4af2-41e6-afe0-2005c1b594a8"
      },
      "source": [
        "# Remove questions and answers that are shorter than 1 word and longer than 20 words. \n",
        "min_line_length = 2\n",
        "max_line_length = 20\n",
        "\n",
        "# Filter out the questions that are too short/long\n",
        "short_questions_temp = []\n",
        "short_answers_temp = []\n",
        "\n",
        "for i, question in enumerate(clean_questions):\n",
        "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
        "        short_questions_temp.append(question)\n",
        "        short_answers_temp.append(clean_answers[i])\n",
        "\n",
        "# Filter out the answers that are too short/long\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "\n",
        "for i, answer in enumerate(short_answers_temp):\n",
        "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
        "        short_answers.append(answer)\n",
        "        short_questions.append(short_questions_temp[i])\n",
        "        \n",
        "print(len(short_questions))\n",
        "print(len(short_answers))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95845\n",
            "95845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "85389732fd564d7b160339bb26d0d1b5dea41fe7",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cDt5gyIVnwP",
        "outputId": "e34da031-2132-407a-f1c0-fb9a3ac2cb72"
      },
      "source": [
        "r = np.random.randint(1,len(short_questions))\n",
        "\n",
        "for i in range(r, r+3):\n",
        "    print(short_questions[i])\n",
        "    print(short_answers[i])\n",
        "    print()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "geez, agent desmond, it is threethirty in the morning. where are we going to sleep?\n",
            "it is a piece of paper with the letter t imprinted on it. take a look.\n",
            "\n",
            "what is it?\n",
            "agent desmond, would you hold the finger for me. there's something up there.\n",
            "\n",
            "there appears to be a contusion under the ring finger of her left hand.\n",
            "cole said she was 17.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5825a5967e5a04f13b0a755e8ba304c419af4b33",
        "id": "r2y4mrDvVnwP"
      },
      "source": [
        "## Preprocessing for word based model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMXoDBZ2eIwx",
        "outputId": "28d8793a-8603-41f6-a664-362428480801"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "106248b9b0ac7499c2b34cc3ae1c20a2e0ea41c4",
        "id": "_kACyOGFVnwP"
      },
      "source": [
        "#choosing number of samples\n",
        "num_samples = 30000  # Number of samples to train on.\n",
        "short_questions = short_questions[:num_samples]\n",
        "short_answers = short_answers[:num_samples]\n",
        "#tokenizing the qns and answers\n",
        "short_questions_tok = [nltk.word_tokenize(sent) for sent in short_questions]\n",
        "short_answers_tok = [nltk.word_tokenize(sent) for sent in short_answers]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr9fJIzzfIZk"
      },
      "source": [
        "## training data & validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "8b37316d3b68626ed388bbb73863364608027796",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7mK0w77VnwP",
        "outputId": "1eb21012-32a9-4fab-f3b7-665c7fb2fcd7"
      },
      "source": [
        "#train-validation split\n",
        "data_size = len(short_questions_tok)\n",
        "\n",
        "# We will use the first 0-80th %-tile (80%) of data for the training\n",
        "training_input  = short_questions_tok[:round(data_size*(80/100))]\n",
        "training_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\n",
        "training_output = short_answers_tok[:round(data_size*(80/100))]\n",
        "\n",
        "# We will use the remaining for validation\n",
        "validation_input = short_questions_tok[round(data_size*(80/100)):]\n",
        "validation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\n",
        "validation_output = short_answers_tok[round(data_size*(80/100)):]\n",
        "\n",
        "print('training size', len(training_input))\n",
        "print('validation size', len(validation_input))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training size 24000\n",
            "validation size 6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a3aacc0315837faf77599da087f8b3a52d217859",
        "id": "qoKSqYY8VnwP"
      },
      "source": [
        "## Word en/decoding dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "51918567f4f43ae13a8d58a00bda15b1d508b191",
        "id": "_kGMtmrJVnwQ"
      },
      "source": [
        "# Create a dictionary for the frequency of the vocabulary\n",
        "vocab = {}\n",
        "for question in short_questions_tok:\n",
        "    for word in question:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = 1\n",
        "        else:\n",
        "            vocab[word] += 1\n",
        "\n",
        "for answer in short_answers_tok:\n",
        "    for word in answer:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = 1\n",
        "        else:\n",
        "            vocab[word] += 1            "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "30d25056e8f62453bcde0a16c8a7933275f431c9",
        "id": "I-P6WqyIVnwQ"
      },
      "source": [
        "# Remove rare words from the vocabulary.\n",
        "# We will aim to replace fewer than 5% of words with <UNK>\n",
        "# You will see this ratio soon.\n",
        "threshold = 15\n",
        "count = 0\n",
        "for k,v in vocab.items():\n",
        "    if v >= threshold:\n",
        "        count += 1"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "4c980b035173d12d2bf06dafd6410eb5da6023b2",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaIGycmnVnwQ",
        "outputId": "af6cfe20-1c15-4213-b7a6-43825e02c707"
      },
      "source": [
        "print(\"Size of total vocab:\", len(vocab))\n",
        "print(\"Size of vocab we will use:\", count)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of total vocab: 20027\n",
            "Size of vocab we will use: 1925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "d62a629773e34bb7bc1a12508cce820c0e47962d",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgy7X8YcVnwQ",
        "outputId": "343776d9-37dc-44ed-f850-40cb7e11ed4a"
      },
      "source": [
        "#we will create dictionaries to provide a unique integer for each word.\n",
        "WORD_CODE_START = 1\n",
        "WORD_CODE_END=2\n",
        "WORD_CODE_PADDING = 0\n",
        "\n",
        "word_num  = 3 #number 1 & 2  are left for WORD_CODE_START & WORD_CODE_END for model decoder later\n",
        "encoding = {'START': 1, 'END':2}\n",
        "decoding = {1:  'START',2: 'END'}\n",
        "for word, count in vocab.items():\n",
        "    if count >= threshold: #get vocabularies that appear above threshold count\n",
        "        encoding[word] = word_num \n",
        "        decoding[word_num ] = word\n",
        "        word_num += 1\n",
        "\n",
        "print(\"No. of vocab used:\", word_num)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of vocab used: 1928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "de2e46d3826def6b6c94a9827c32118d555fae2e",
        "id": "G8kfPWsoVnwQ"
      },
      "source": [
        "#include unknown token for words not in dictionary\n",
        "decoding[len(encoding)+3] = '<UNK>'\n",
        "encoding['<UNK>'] = len(encoding)+3"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "bccd51e8730e7c3da1234af9480da767f400c230",
        "collapsed": true,
        "id": "5wlChVgcVnwQ"
      },
      "source": [
        "dict_size = word_num+3\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg4U1eKKHmw9"
      },
      "source": [
        "\n",
        "np.save('word2id.npy',encoding)\n",
        "np.save('id2word.npy',decoding)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "7ea3c32a6180ee8fea8b1490b40351897cc426e6",
        "id": "AwQrb39cVnwR"
      },
      "source": [
        "##  Vectorizing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "4de33f91476bd6c50cf7c9322f273079783ceec2",
        "id": "s5N8mcjqVnwR"
      },
      "source": [
        "def transform(encoding, data, vector_size=20):\n",
        "    \"\"\"\n",
        "    :param encoding: encoding dict built by build_word_encoding()\n",
        "    :param data: list of strings\n",
        "    :param vector_size: size of each encoded vector\n",
        "    \"\"\"\n",
        "    transformed_data = np.zeros(shape=(len(data), vector_size))\n",
        "    for i in range(len(data)):\n",
        "        for j in range(min(len(data[i]), vector_size)):\n",
        "            try:\n",
        "                transformed_data[i][j] = encoding[data[i][j]]\n",
        "            except:\n",
        "                transformed_data[i][j] = encoding['<UNK>']\n",
        "    return transformed_data"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "f13007fbe8661c4c2ea4c35fd51b86c4fc220efb",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzOsCecGVnwR",
        "outputId": "b750c6f2-f986-4a3f-f994-d751bb4f45fc"
      },
      "source": [
        "#encoding training set\n",
        "encoded_training_input = transform(\n",
        "    encoding, training_input, vector_size=INPUT_LENGTH)\n",
        "encoded_training_output = transform(\n",
        "    encoding, training_output, vector_size=OUTPUT_LENGTH)\n",
        "\n",
        "print('encoded_training_input', encoded_training_input.shape)\n",
        "print('encoded_training_output', encoded_training_output.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded_training_input (24000, 20)\n",
            "encoded_training_output (24000, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "cbbf370a5d0640a4dea6b6d9237a5917978b92a8",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHlp3zYTVnwR",
        "outputId": "a3ddfc50-6247-456c-d92c-0e37fd6f2cde"
      },
      "source": [
        "#encoding validation set\n",
        "encoded_validation_input = transform(\n",
        "    encoding, validation_input, vector_size=INPUT_LENGTH)\n",
        "encoded_validation_output = transform(\n",
        "    encoding, validation_output, vector_size=OUTPUT_LENGTH)\n",
        "\n",
        "print('encoded_validation_input', encoded_validation_input.shape)\n",
        "print('encoded_validation_output', encoded_validation_output.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded_validation_input (6000, 20)\n",
            "encoded_validation_output (6000, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b52f0368fecf8ac79903927799fe67c104d8e518",
        "id": "NBjyJtlUVnwR"
      },
      "source": [
        "##  Model Building\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uq6F7KEfwRM"
      },
      "source": [
        "###   Sequence-to-Sequence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "e8b5afc77468e4e263b3fc7ff62a2e6952a80e97",
        "id": "aKemQr3bVnwR"
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "306b1fb012febd74dd40840c0588bdd8503ba65b",
        "id": "w8uy32BjVnwR"
      },
      "source": [
        "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
        "decoder_input = Input(shape=(OUTPUT_LENGTH,))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yewQOOcbfhiM"
      },
      "source": [
        "### Using glove for embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62uu3k4FN0OG",
        "outputId": "e556e53e-5588-48a5-aa57-0cd3bb8d0dfc"
      },
      "source": [
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('/content/drive/MyDrive/glove/glove.twitter.27B.25d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((dict_size, 25))\n",
        "for word, i in encoding.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector\n",
        "\n",
        "embed_layer = Embedding(input_dim=dict_size, output_dim=25,input_length=INPUT_LENGTH, trainable=True, mask_zero=True)\n",
        "embed_layer.build((None,))\n",
        "embed_layer.set_weights([embedding_matrix])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1193515 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "d91367ec6c706059c4b520011fdec6e1051db38d",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIiHUnF0VnwR",
        "outputId": "d8c4fc2c-cde7-4e30-ecf3-dbd9439b6a5a"
      },
      "source": [
        "encoder = embed_layer(encoder_input)\n",
        "encoder = LSTM(512, return_sequences=True, unroll=True)(encoder)\n",
        "encoder_last = encoder[:,-1,:]\n",
        "\n",
        "print('encoder', encoder)\n",
        "print('encoder_last', encoder_last)\n",
        "\n",
        "decoder = embed_layer(decoder_input)\n",
        "decoder = LSTM(512, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n",
        "\n",
        "print('decoder', decoder)\n",
        "\n",
        "# For the plain Sequence-to-Sequence, we produced the output from directly from decoder\n",
        "# output = TimeDistributed(Dense(output_dict_size, activation=\"softmax\"))(decoder)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder KerasTensor(type_spec=TensorSpec(shape=(None, 20, 512), dtype=tf.float32, name=None), name='lstm/transpose_2:0', description=\"created by layer 'lstm'\")\n",
            "encoder_last KerasTensor(type_spec=TensorSpec(shape=(None, 512), dtype=tf.float32, name=None), name='tf.__operators__.getitem/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem'\")\n",
            "decoder KerasTensor(type_spec=TensorSpec(shape=(None, 22, 512), dtype=tf.float32, name=None), name='lstm_1/transpose_2:0', description=\"created by layer 'lstm_1'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2feb58fa60a8ff335b218114808aa2d3e4d37461",
        "id": "tVaHT7YlVnwS"
      },
      "source": [
        "### Attention Mechanism\n",
        "Reference: Effective Approaches to Attention-based Neural Machine Translation's Global Attention with Dot-based scoring function (Section 3, 3.1) https://arxiv.org/pdf/1508.04025.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "89c5ce99b6b7e42bf02559816dc56cdc9a29423e",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwxcTxB5VnwS",
        "outputId": "23fe43d0-47fc-464a-afa4-658a6f499782"
      },
      "source": [
        "\n",
        "# Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
        "# Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
        "attention = dot([decoder, encoder], axes=[2, 2])\n",
        "attention = Activation('softmax', name='attention')(attention)\n",
        "print('attention', attention)\n",
        "\n",
        "context = dot([attention, encoder], axes=[2,1])\n",
        "print('context', context)\n",
        "\n",
        "decoder_combined_context = concatenate([context, decoder])\n",
        "print('decoder_combined_context', decoder_combined_context)\n",
        "\n",
        "# Has another weight + tanh layer as described in equation (5) of the paper\n",
        "output = TimeDistributed(Dense(512, activation=\"tanh\"))(decoder_combined_context)\n",
        "output = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(output)\n",
        "print('output', output)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention KerasTensor(type_spec=TensorSpec(shape=(None, 22, 20), dtype=tf.float32, name=None), name='attention/Softmax:0', description=\"created by layer 'attention'\")\n",
            "context KerasTensor(type_spec=TensorSpec(shape=(None, 22, 512), dtype=tf.float32, name=None), name='dot_1/MatMul:0', description=\"created by layer 'dot_1'\")\n",
            "decoder_combined_context KerasTensor(type_spec=TensorSpec(shape=(None, 22, 1024), dtype=tf.float32, name=None), name='concatenate/concat:0', description=\"created by layer 'concatenate'\")\n",
            "output KerasTensor(type_spec=TensorSpec(shape=(None, 22, 1931), dtype=tf.float32, name=None), name='time_distributed_1/Reshape_1:0', description=\"created by layer 'time_distributed_1'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "257dcf12770f8928f91754044d3efc5aff1ec296",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b86z-sTqVnwS",
        "outputId": "3c6db513-4e83-427f-80a2-79eb97378087"
      },
      "source": [
        "model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 22)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           multiple             48275       input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 20, 512)      1101824     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem (Slici (None, 512)          0           lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 22, 512)      1101824     embedding[1][0]                  \n",
            "                                                                 tf.__operators__.getitem[0][0]   \n",
            "                                                                 tf.__operators__.getitem[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 22, 20)       0           lstm_1[0][0]                     \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "attention (Activation)          (None, 22, 20)       0           dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 22, 512)      0           attention[0][0]                  \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 22, 1024)     0           dot_1[0][0]                      \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 22, 512)      524800      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 22, 1931)     990603      time_distributed[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 3,767,326\n",
            "Trainable params: 3,767,326\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwuSsFlWtX0N"
      },
      "source": [
        "def batch_generator(X,y,out,steps,batch_size):\n",
        "    idx=0\n",
        "    while True: \n",
        "          batch_x = np.array(X[idx * batch_size : (idx+1) * batch_size])\n",
        "          batch_y = np.array(y[idx * batch_size : (idx+1) * batch_size])\n",
        "          output_y = np.array(out[idx * batch_size : (idx+1) * batch_size])\n",
        "          yield [batch_x,batch_y],output_y ## Yields data\n",
        "          if idx<steps:\n",
        "              # print(idx,steps)\n",
        "              idx+=1\n",
        "          # else:\n",
        "          #     # idx=0\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "09b61276a10c14838eb09db124a229ac58c61133",
        "collapsed": true,
        "id": "WsQglTwQVnwS"
      },
      "source": [
        "training_encoder_input = encoded_training_input\n",
        "training_decoder_input = np.zeros_like(encoded_training_output)\n",
        "training_decoder_input[:, 1:-1] = encoded_training_output[:,:-2]\n",
        "training_decoder_input[:, 0] = WORD_CODE_START\n",
        "training_decoder_input[:, -1] = WORD_CODE_END\n",
        "training_decoder_output = np.eye(dict_size)[encoded_training_output.astype('int')]\n",
        "\n",
        "validation_encoder_input = encoded_validation_input\n",
        "validation_decoder_input = np.zeros_like(encoded_validation_output)\n",
        "validation_decoder_input[:, 1:-1] = encoded_validation_output[:,:-2]\n",
        "validation_decoder_input[:, 0] = WORD_CODE_START\n",
        "validation_decoder_input[:, -1] = WORD_CODE_END\n",
        "\n",
        "validation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpbSGq6AvIz7"
      },
      "source": [
        "BATCH_SIZE =128\n",
        "EPOCHS = 25\n",
        "# STEPS=(np.ceil((len(training_encoder_input) / float(BATCH_SIZE))-1)).astype(np.int)\n",
        "\n",
        "steps_per_epoch = len(training_encoder_input)//BATCH_SIZE\n",
        "validation_steps=len(validation_encoder_input)//BATCH_SIZE\n",
        "\n",
        "my_training_batch_generator=batch_generator(training_encoder_input,training_decoder_input,training_decoder_output,steps_per_epoch,BATCH_SIZE)\n",
        "my_validation_batch_generator=batch_generator(validation_encoder_input,validation_decoder_input,validation_decoder_output,validation_steps,BATCH_SIZE)\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD9NSHRIZQfr",
        "outputId": "306cc3ae-709f-4be8-a8f8-cc1b1dd8fcb6"
      },
      "source": [
        "model.fit_generator(my_training_batch_generator,\n",
        "          #validation_split=0.05,\n",
        "          steps_per_epoch=steps_per_epoch, epochs=EPOCHS,verbose=1,\n",
        "          validation_data=my_validation_batch_generator,validation_steps=validation_steps)\n",
        "\n",
        "model.save('chatboot_model.h5')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "187/187 [==============================] - 345s 2s/step - loss: 2.3034 - val_loss: 2.0700\n",
            "Epoch 2/25\n",
            "187/187 [==============================] - 210s 1s/step - loss: 0.0554 - val_loss: 4.8956\n",
            "Epoch 3/25\n",
            "187/187 [==============================] - 209s 1s/step - loss: 0.0011 - val_loss: 5.3427\n",
            "Epoch 4/25\n",
            "187/187 [==============================] - 210s 1s/step - loss: 0.0010 - val_loss: 5.4895\n",
            "Epoch 5/25\n",
            "187/187 [==============================] - 213s 1s/step - loss: 0.0010 - val_loss: 5.5464\n",
            "Epoch 6/25\n",
            "187/187 [==============================] - 218s 1s/step - loss: 0.0010 - val_loss: 5.5786\n",
            "Epoch 7/25\n",
            "187/187 [==============================] - 215s 1s/step - loss: 0.0010 - val_loss: 5.6041\n",
            "Epoch 8/25\n",
            "187/187 [==============================] - 217s 1s/step - loss: 0.0010 - val_loss: 5.6278\n",
            "Epoch 9/25\n",
            "187/187 [==============================] - 211s 1s/step - loss: 0.0010 - val_loss: 5.6389\n",
            "Epoch 10/25\n",
            "187/187 [==============================] - 206s 1s/step - loss: 0.0010 - val_loss: 5.6524\n",
            "Epoch 11/25\n",
            "187/187 [==============================] - 207s 1s/step - loss: 0.0010 - val_loss: 5.6689\n",
            "Epoch 12/25\n",
            "187/187 [==============================] - 219s 1s/step - loss: 0.0010 - val_loss: 5.6747\n",
            "Epoch 13/25\n",
            "187/187 [==============================] - 215s 1s/step - loss: 0.0010 - val_loss: 5.6808\n",
            "Epoch 14/25\n",
            "187/187 [==============================] - 218s 1s/step - loss: 0.0010 - val_loss: 5.6910\n",
            "Epoch 15/25\n",
            "187/187 [==============================] - 219s 1s/step - loss: 0.0010 - val_loss: 5.6954\n",
            "Epoch 16/25\n",
            "187/187 [==============================] - 220s 1s/step - loss: 0.0010 - val_loss: 5.7030\n",
            "Epoch 17/25\n",
            "187/187 [==============================] - 214s 1s/step - loss: 0.0010 - val_loss: 5.7104\n",
            "Epoch 18/25\n",
            "187/187 [==============================] - 212s 1s/step - loss: 0.0010 - val_loss: 5.7181\n",
            "Epoch 19/25\n",
            "187/187 [==============================] - 217s 1s/step - loss: 0.0010 - val_loss: 5.7241\n",
            "Epoch 20/25\n",
            "187/187 [==============================] - 217s 1s/step - loss: 0.0010 - val_loss: 5.7289\n",
            "Epoch 21/25\n",
            "187/187 [==============================] - 217s 1s/step - loss: 0.0010 - val_loss: 5.7348\n",
            "Epoch 22/25\n",
            "187/187 [==============================] - 221s 1s/step - loss: 0.0010 - val_loss: 5.7400\n",
            "Epoch 23/25\n",
            "187/187 [==============================] - 216s 1s/step - loss: 0.0010 - val_loss: 5.7430\n",
            "Epoch 24/25\n",
            "187/187 [==============================] - 217s 1s/step - loss: 0.0010 - val_loss: 5.7498\n",
            "Epoch 25/25\n",
            "187/187 [==============================] - 225s 1s/step - loss: 0.0010 - val_loss: 5.7509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "3d43e174878dfbef64900b7f043d7bb41f458309",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diG4_jNKVnwS",
        "outputId": "3cc6d3df-77d0-4286-bc2f-bbffd6f813d7"
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "model.fit_generator(my_training_batch_generator,\n",
        "          #validation_split=0.05,\n",
        "          steps_per_epoch=steps_per_epoch, epochs=EPOCHS,verbose=1,\n",
        "          validation_data=my_validation_batch_generator,validation_steps=validation_steps)\n",
        "\n",
        "model.save('chatboot_model.h5')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "187/187 [==============================] - 363s 2s/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 2/10\n",
            "187/187 [==============================] - 218s 1s/step - loss: 9.7921e-04 - val_loss: 0.0021\n",
            "Epoch 3/10\n",
            "187/187 [==============================] - 216s 1s/step - loss: 1.4661e-04 - val_loss: 0.0029\n",
            "Epoch 4/10\n",
            "187/187 [==============================] - 215s 1s/step - loss: 9.2506e-06 - val_loss: 0.0030\n",
            "Epoch 5/10\n",
            "187/187 [==============================] - 214s 1s/step - loss: 4.2064e-06 - val_loss: 0.0031\n",
            "Epoch 6/10\n",
            "187/187 [==============================] - 214s 1s/step - loss: 2.8988e-06 - val_loss: 0.0032\n",
            "Epoch 7/10\n",
            "187/187 [==============================] - 214s 1s/step - loss: 2.3045e-06 - val_loss: 0.0032\n",
            "Epoch 8/10\n",
            "187/187 [==============================] - 211s 1s/step - loss: 1.9719e-06 - val_loss: 0.0033\n",
            "Epoch 9/10\n",
            "187/187 [==============================] - 213s 1s/step - loss: 1.7623e-06 - val_loss: 0.0033\n",
            "Epoch 10/10\n",
            "187/187 [==============================] - 214s 1s/step - loss: 1.6200e-06 - val_loss: 0.0033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "874f0398f10f48cce993bcd9aa7b5e0644b6f292",
        "id": "BnNe9Jx_VnwS"
      },
      "source": [
        "## 3. Model testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "d0e3217f3d250c253dd457f192f54f8cd67c630f",
        "collapsed": true,
        "id": "xTZDLoM2VnwS"
      },
      "source": [
        "def prediction(raw_input):\n",
        "    clean_input = clean_text(raw_input)\n",
        "    input_tok = [nltk.word_tokenize(clean_input)]\n",
        "    input_tok = [input_tok[0][::-1]]  #reverseing input seq\n",
        "    encoder_input = transform(encoding, input_tok, 20)\n",
        "    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
        "    decoder_input[:,0] = WORD_CODE_START\n",
        "    decoder_input[:,-1] = WORD_CODE_END\n",
        "\n",
        "    for i in range(1, OUTPUT_LENGTH):\n",
        "        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
        "        decoder_input[:,i] = output[:,i]\n",
        "    return output\n",
        "\n",
        "def decode(decoding, vector):\n",
        "    \"\"\"\n",
        "    :param decoding: decoding dict built by word encoding\n",
        "    :param vector: an encoded vector\n",
        "    \"\"\"\n",
        "    text = ''\n",
        "    for i in vector:\n",
        "        if i == 0:\n",
        "            break\n",
        "        text += ' '\n",
        "        text += decoding[i]\n",
        "    return text"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "16e6ece57e4c02321c58176e5471f37ad1170837",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0LhPuf1VnwS",
        "outputId": "8214285e-dd03-43d0-b6db-af60131cfd94"
      },
      "source": [
        "for i in range(20):\n",
        "    seq_index = np.random.randint(1, len(short_questions))\n",
        "    output = prediction(short_questions[seq_index])\n",
        "    print ('Q:', short_questions[seq_index])\n",
        "    print ('A:', decode(decoding, output[0]))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: not yet\n",
            "A:  i saw her die . she was shot . with this gun .\n",
            "Q: where were you at twelve o'clock last night?\n",
            "A:  mrs. grant , governor ... i will not hurt you .\n",
            "Q: maybe it is supposed to end now. maybe god would not have it any other way.\n",
            "A:  i appreciate that . but i am also sorry to am of my street .\n",
            "Q: ai not it the truth.\n",
            "A:  <UNK> , i have <UNK> hair . i am thinking of <UNK> the speech .\n",
            "Q: i do not care what you have got started. do you want to go?\n",
            "A:  i got your message . where is craig ?\n",
            "Q: yes, sir.\n",
            "A:  look ... i have ... i have got a problem . a big problem ...\n",
            "Q: yyyy... yyye... yyyess.\n",
            "A:  keep , what ?\n",
            "Q: yes...yes...i will explain it all. just put the gun down.\n",
            "A:  get on !\n",
            "Q: well, we would like to find out something about him. what does he do for a living?\n",
            "A:  better in san <UNK> ? more <UNK> there ? what ?\n",
            "Q: thank you... mr. shaw.\n",
            "A:  <UNK> . and i want not you .\n",
            "Q: monsieur, insofar as it is in my power\n",
            "A:  but i want to make some <UNK> . get <UNK> <UNK> away will not not you ?\n",
            "Q: that dog woke me. i lay there for a while, then decided to get up.\n",
            "A:  when did way to way to get into more trouble . try to have <UNK> at home .\n",
            "Q: i told you we do not know for certain.\n",
            "A:  i need your help .\n",
            "Q: he was not black! he was... tan! weathered!\n",
            "A:  shut , no <UNK> tonight ? what , what there daughter ?\n",
            "Q: then find me somebody else, before they come looking.\n",
            "A:  i need your help . but i am also sorry to your , <UNK> . mrs. . say .\n",
            "Q: look, i really do not have that much time...\n",
            "A:  forget about it .\n",
            "Q: you will.\n",
            "A:  good luck .\n",
            "Q: has he hinted around?\n",
            "A:  <UNK> , i want not you . she was shot . this is gun .\n",
            "Q: a deer?\n",
            "A:  where would you come from ?\n",
            "Q: you said it.\n",
            "A:  that ai not gon na work .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrN4VvIiV56K",
        "outputId": "a36f3119-b25c-4dc1-bf7d-9bc6420b9444"
      },
      "source": [
        "for i in range(6):\n",
        "    seq_index = np.random.randint(1, len(short_questions))\n",
        "    output = prediction(short_questions[seq_index])\n",
        "    print ('Q:', short_questions[seq_index])\n",
        "    print ('A:', decode(decoding, output[0]))\n",
        "    print ('RA:', short_answers[seq_index])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: rome is going to pay an allotment to the german tribes on an annual basis.\n",
            "A:  get on .\n",
            "RA: what deal?\n",
            "Q: purely personal. i believe you might enjoy one another.\n",
            "A:  mr. <UNK> time . but take it easy on me , girl .\n",
            "RA: if you do not want me to pose for him, why do you want me to meet him?\n",
            "Q: it is a post all vienna seeks. if you want it for your husband, come tonight.\n",
            "A:  come on , baby , let 's go in the house .\n",
            "RA: is not it obvious?\n",
            "Q: is it the truth?\n",
            "A:  i saw her die . she was shot .\n",
            "RA: my heart weeps.\n",
            "Q: what are you talking about, bob?\n",
            "A:  its ' ... ah ... about my daughter ... .\n",
            "RA: if you are that worried, maybe we should just steal one.\n",
            "Q: 'night miss jenny do not let the bedbugs bite.\n",
            "A:  if you were here to hurt you i would have done it already .\n",
            "RA: just a man. goodnight pearl, sleep tight and do not let the bedbugs bite.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0d5742477af42914841bb341532f7e9e01c15815",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKQ6mMMQVnwS",
        "outputId": "8a1424fd-04d8-450a-a1df-6617763a4f5e"
      },
      "source": [
        "raw_input = input()\n",
        "output = prediction(raw_input)\n",
        "print (decode(decoding, output[0]))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello mr. shaw\n",
            " i have you come from <UNK> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eajNIeL9gSK8",
        "outputId": "dc5e5188-0d8e-4cbc-8fee-2849a19f6f92"
      },
      "source": [
        "out_last_ques= prediction(last_que)\n",
        "print(last_que)\n",
        "print(decode(decoding, out_last_ques[0]))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "colonel durnford... william vereker. i hear you have been seeking officers?\n",
            " somebody left me a message . well where is craig and dayday ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "9c5007896f6a9aaf3395fc82447ca2bb5cc57b65",
        "id": "eaL-WCM1VnwK"
      },
      "source": [
        "## Resources\n",
        "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.htmll"
      ]
    }
  ]
}